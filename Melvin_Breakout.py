# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/136PCwCoLakouqcvbjyCKtOSUUDfZMbaX
"""

!pip install stable-baselines3[extra]
!pip install gymnasium

import os
from stable_baselines3.common.callbacks import BaseCallback
from stable_baselines3.common import env_checker
from stable_baselines3.common.vec_env import VecFrameStack
from stable_baselines3.common.evaluation import evaluate_policy
from stable_baselines3.common.env_util import make_atari_env
from stable_baselines3 import DQN
import gymnasium as gym

class TrainAndLoggingCallback(BaseCallback):
    def __init__(self, check_freq, save_path, verbose=1):
        super(TrainAndLoggingCallback, self).__init__(verbose)
        self.check_freq = check_freq
        self.save_path = save_path

    def _init_callback(self):

        if self.save_path is not None:
            os.makedirs(self.save_path, exist_ok=True)

    def _on_step(self):
        if self.n_calls % self.check_freq == 0:
            model_path = os.path.join(self.save_path, f'best_model_{self.n_calls}')
            self.model.save(model_path)
        return True

CHECKPOINT_DIR = './train/'
LOG_DIR = './logs/'
window = 4
env = make_atari_env("ALE/Breakout-v5", n_envs=1, monitor_dir=LOG_DIR)
# Frame-stacking with 4 frames
vec_env = VecFrameStack(env, n_stack=window)

callback = TrainAndLoggingCallback(check_freq=100000, save_path=CHECKPOINT_DIR)
newmodel = DQN('CnnPolicy', vec_env, tensorboard_log=LOG_DIR, verbose=1,
            buffer_size=50000, batch_size=64, learning_starts=10000, gamma=0.95,
            exploration_fraction = 0.5, exploration_final_eps=0.1)

newmodel.learn(total_timesteps=1000000, callback=callback, log_interval=1000)

model = DQN.load("/content/best_model_1000000.zip", print_system_info=True)
window = 4

import imageio
import numpy as np

env = make_atari_env("ALE/Breakout-v5", n_envs=1)
window=4
# Frame-stacking with 4 frames
vec_env = VecFrameStack(env, n_stack=window)
model.set_env(vec_env)
imgs = []
episodes = 1
for episode in range(1, episodes+1):
    obs = vec_env.reset()

    score = 0

    while True:
        action, _states = model.predict(obs, deterministic=False)
        obs, rewards, dones, info = vec_env.step(action)
#         print(info[0]['lives'])
        score += rewards[0]
        vec_env.render("rgb_array")
        imgs.append(model.env.render(mode='rgb_array'))
        if info[0]['lives'] == 0 and dones[0]:
            break

    print('Episode:{} Score:{}'.format(episode, score))
vec_env.close()
del vec_env

imageio.mimsave('breakout.gif', [np.array(img) for i, img in enumerate(imgs)])

from IPython.display import Image, display
display(Image(data=open('/content/breakout.gif','rb').read(), format='gif'))

env = make_atari_env("ALE/Breakout-v5", n_envs=1)
window=4
# Frame-stacking with 4 frames
vec_env = VecFrameStack(env, n_stack=window)
rew_mean, rew_std = evaluate_policy(model, vec_env, n_eval_episodes=100, render=True)

rew_mean, rew_std